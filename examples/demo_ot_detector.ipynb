{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Wasserstein distance using Optimal Transport (OT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Computational Optimal Transport\" https://arxiv.org/abs/1803.00567\n",
    "\n",
    "\"POT: Python Optimal Transport\" https://jmlr.org/papers/v22/20-451.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Transport (OT) is a field of mathematics which studies the geometry of probability spaces. Among its many contributions, OT provides a principled way to compare and align probability distributions by taking into account the underlying geometry of the\n",
    "considered metric space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Transport (OT) is a mathematical problem that was first introduced by Gaspard Monge in 1781. It addresses the task of determining the most efficient method for transporting mass from one distribution to another. In this problem, the cost associated with moving a unit of mass from one position to another is referred to as the ground cost. The primary objective of OT is to minimize the total cost incurred when moving one mass distribution onto another. The optimization problem can be expressed for two distributions $\\mu_s$ and $\\mu_t$ as\n",
    "$$\n",
    "\\min_{\\sigma \\in \\text{Perm}(n)} \\frac{1}{n} \\sum_{i=1}^n \\textbf{C}_{i,\\sigma(i)}\n",
    "$$\n",
    "\n",
    "where $\\textbf{C}_{\\cdot, \\cdot}$ is the ground cost and the constraint $m_{\\#} \\mu_s=\\mu_t$ ensures that $\\mu_s$ is completely transported to $\\mu_t$. Where $T_{\\#} \\mu_s = \\mu_s(T^{-1}(B)) = u_{t}(B)$ with $T$ as a trasportation matrix between $\\mu_s$ and $\\mu_t$ at point $B$. Due to the inherent constraint in the problem, solving Optimal Transport (OT) can be quite challenging. As a result, in practical applications dealing with discrete distributions, a more manageable approach known as a linear program has been employed as a substitute. This approach corresponds to the Kantorovitch formulation, where the original Monge mapping, denoted as $m$ is replaced by a joint distribution represented by an OT matrix.\n",
    "From the optimization problem described above, we can identify two primary components of the OT solution that have practical applications:\n",
    "- The optimal value (Wasserstein distance): This quantifies the similarity between distributions. It is used to measure the dissimilarity or similarity between datasets or distributions. The Wasserstein distance represents the optimal value obtained from solving the OT problem.\n",
    "- The optimal mapping (Monge mapping or OT matrix): This determines the correspondences between the distributions. It describes how the mass is transported between the source and target distributions. The optimal mapping can be utilized to transfer knowledge or information between distributions.\n",
    "\n",
    "In the first case, OT is employed to assess the similarity between distributions or datasets. Here, the Wasserstein distance, which is the optimal value obtained from solving the OT problem, is used as a measure of similarity. In the second case, the focus lies on understanding the specific manner in which mass is transferred between distributions, represented by the mapping. This mapping can be leveraged to facilitate the transfer of knowledge or information between the distributions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein distance between distributions\n",
    "\n",
    "Optimal Transport (OT) is commonly employed as a means to quantify the similarity between distributions, particularly when the distributions have distinct supports. In scenarios where the supports of the distributions are disjoint, OT-based Wasserstein distances offer favorable comparisons to well-known f-divergences such as the Kullback-Leibler divergence, Jensen-Shannon divergence, and Total Variation distance.\n",
    "\n",
    "One notable aspect that makes OT valuable for data science applications is its ability to compute meaningful sub-gradients of the Wasserstein distance. This feature enhances its efficiency as a tool for measuring and optimizing similarity between empirical distributions.\n",
    "\n",
    "The machine learning (ML) literature has seen numerous contributions utilizing OT as an approach. For instance, in the training of Generative Adversarial Networks (GANs), OT has been utilized to tackle the issue of vanishing gradients, which can hinder the learning process. Additionally, OT has been employed to identify discriminant or robust subspaces within datasets, offering useful insights. Moreover, the Wasserstein distance has found application in measuring similarity between word embeddings of documents, as well as comparing signals or spectra."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OT for mapping estimation\n",
    "\n",
    "One fascinating aspect of the Optimal Transport (OT) problem is the inherent OT mapping. When computing the optimal transport between discrete distributions, one of the outputs is the OT matrix, which provides correspondences between the samples in each distribution.\n",
    "\n",
    "This correspondence is estimated based on the OT criterion and is obtained in a non-supervised manner. This characteristic makes it particularly intriguing for problems involving dataset transfer. OT has been employed, for instance, in performing color transfer between images or in the context of domain adaptation, where knowledge or information is transferred between different datasets.\n",
    "\n",
    "Furthermore, more recent applications have explored the extension of OT, known as Gromov-Wasserstein, to establish correspondences between languages using word embeddings. This utilization of OT enables the identification of connections or similarities between languages based on the distributional properties of word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kantorovich optimal transport problem\n",
    "\n",
    "This is the most typical OT problem. It seeks an optimal coupling $\\boldsymbol{T}$ which minimizes the displacement cost of a discrete measure $\\boldsymbol{a}$ to a discrete measure $\\boldsymbol{b}$ with respect to a ground cost $\\boldsymbol{M} \\in \\mathbb{R}^{n_{1} \\times n_{2}}$. In order to be a transport plan, $\\boldsymbol{T}$ must be part of the set $\\Pi(\\mathbf{a}, \\mathbf{b})=\\left\\{\\boldsymbol{T} \\geq \\mathbf{0}, \\boldsymbol{T} \\mathbf{1}_{n_{2}}=\\boldsymbol{a}, \\boldsymbol{T}^{\\top} \\mathbf{1}_{n_{1}}=\\boldsymbol{b}\\right\\}$. When the ground cost is a metric, the optimal value of the OT problem is also a metric (Rubner et al., 2000; Cuturi and Avis, 2014) and is called the Wasserstein distance. In this discrete case, the OT problem is defined as\n",
    "\n",
    "$$\n",
    "W_{M}(\\boldsymbol{a}, \\boldsymbol{b})=\\min _{\\boldsymbol{T} \\in \\Pi(\\mathbf{a}, \\mathbf{b})}\\langle\\boldsymbol{T}, \\boldsymbol{M}\\rangle\n",
    "$$\n",
    "\n",
    "which is a linear program. The optimization problem above is often adapted to include a regularization term for the transport plan $\\boldsymbol{T}$, such as entropic regularization (Cuturi, 2013) or squared L2. For the entropic regularized OT problem, one may use the Sinkhorn Knopp algorithm (or variants), or stochastic optimization algorithms. POT has a simple syntax to solve these problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving optimal transport\n",
    "\n",
    "The optimal transport problem between discrete distributions is often expressed as\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\gamma^*=\\arg \\min _{\\gamma \\in \\mathbb{R}_{+}^{m \\times n}} \\sum_{i, j} \\gamma_{i, j} M_{i, j} \\\\\n",
    "\\text { s.t. } \\gamma 1=a ; \\gamma^T 1=b ; \\gamma \\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "where:\n",
    "- $M \\in \\mathbb{R}_{+}^{m \\times n}$ is the metric cost matrix defining the cost to move mass from bin $a_i$ to bin $b_j$.\n",
    "- $a$ and $b$ are histograms on the simplex (positive, sum to 1) that represent the weights of each samples in the source an target distributions.\n",
    "Solving the linear program above can be done using the function ot.emd that will return the optimal transport matrix $\\gamma^*$ :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The necessity of usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between the MDSS and OT detectors is the range of applicability. MDSS scanner can in linear time decide, what the most anomalous subset is, while in the case of OT detector has a narrower but more accurate field of use. The method used can only account for 1-dimensional histograms as precisely described in \"POT: Python Optimal Transport\" https://jmlr.org/papers/v22/20-451.html. This means there is no possibility yet to handle datasets of more than 1 feature, therefore it is a sort of toy example to build something bigger in the next steps. For more dimensions, there is a prospect for the Sinkhorn algorithm, which will be implemented in the nearest future.\n",
    "\n",
    "The matrix that is obtained from the emd method is called a transport plan in the OT framework (the gamma matrix in https://pythonot.github.io/all.html?highlight=emd#ot.emd). It is the matrix which minimizes the transportation cost, and workable to evaluate the actual Wasserstein distance by taking the Frobenius product (see here for instance: https://en.wikipedia.org/wiki/Frobenius_inner_product) with the metric cost matrix, M. The testing in 1-dimension can be conducted by getting the same by computing ot.wasserstein_1d, although there is plans to change the way the matrix M is currently defined."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Optimal Transport supports only scoring function *Optimal Transport*.\n",
    "Note, non-parametric scoring functions can only be used for datasets where the expectations are constant or none.\n",
    "\n",
    "The type of outcomes must be provided using the mode keyword argument. The definition for the four types of outcomes supported are provided below:\n",
    "- Ordinal: Multiclass outcomes that are ranked in a specific order. Outcomes must be positive integers.\n",
    "- Binary: Yes/no outcomes. Outcomes must 0 or 1.\n",
    "- Continuous: Continuous outcomes. Outcomes could be any real number.\n",
    "- Nominal: Multiclass outcomes with no rank or order between them. Outcomes must be a finite set of integers with dimensionality <= 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.detectors.ot_detector import ot_bias_scan\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll demonstrate finding the optimal transport matrix with ot_bias_scan using the compas dataset. We can specify subgroups to be scored or scan for the most anomalous subgroup. Bias scan allows us to decide if we aim to identify bias as `higher` than expected probabilities or `lower` than expected probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compas Dataset\n",
    "This is a binary classification use case where the favorable label is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "dataset_orig = load_preproc_data_compas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the categorical features one-hot encoded so we'll modify the dataset to convert them back \n",
    "to the categorical featues because scanning one-hot encoded features may find subgroups that are not meaningful eg. a subgroup with 2 race values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_df = pd.DataFrame(dataset_orig.features, columns=dataset_orig.feature_names)\n",
    "\n",
    "age_cat = np.argmax(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45', \n",
    "                                     'age_cat=Greater than 45']].values, axis=1).reshape(-1, 1)\n",
    "priors_count = np.argmax(dataset_orig_df[['priors_count=0', 'priors_count=1 to 3', \n",
    "                                          'priors_count=More than 3']].values, axis=1).reshape(-1, 1)\n",
    "c_charge_degree = np.argmax(dataset_orig_df[['c_charge_degree=F', 'c_charge_degree=M']].values, axis=1).reshape(-1, 1)\n",
    "\n",
    "features = np.concatenate((dataset_orig_df[['sex', 'race']].values, age_cat, priors_count, \\\n",
    "                           c_charge_degree, dataset_orig.labels), axis=1)\n",
    "feature_names = ['sex', 'race', 'age_cat', 'priors_count', 'c_charge_degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features, columns=feature_names + ['two_year_recid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  race  age_cat  priors_count  c_charge_degree  two_year_recid\n",
       "0  0.0   0.0      1.0           0.0              0.0             1.0\n",
       "1  0.0   0.0      0.0           2.0              0.0             1.0\n",
       "2  0.0   1.0      1.0           2.0              0.0             1.0\n",
       "3  1.0   1.0      1.0           0.0              1.0             0.0\n",
       "4  0.0   1.0      1.0           0.0              0.0             0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (\"sex\" parameter)\n",
    "We'll train a simple classifier to predict the probability of the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "race\n",
      "age_cat\n",
      "priors_count\n",
      "c_charge_degree\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df0 = df[df['sex'] == 0]\n",
    "df1 = df[df['sex'] == 1]\n",
    "\n",
    "\n",
    "X0 = df0.drop('two_year_recid', axis = 1)\n",
    "golden_standart0 = df0['two_year_recid']\n",
    "clf0 = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2')\n",
    "clf0.fit(X0, golden_standart0)\n",
    "\n",
    "X1 = df1.drop('two_year_recid', axis = 1)\n",
    "golden_standart1 = df1['two_year_recid']\n",
    "clf1 = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2')\n",
    "clf1.fit(X1, golden_standart1)\n",
    "\n",
    "for col in X0.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier0 = pd.Series(clf0.predict_proba(X0)[:,0])\n",
    "classifier1 = pd.Series(clf1.predict_proba(X1)[:,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring bias with respect to sex\n",
    "Using ot_bias scan we obtained results evaluated for optimal transport between golden_standarts and classifiers. Their difference shows the measured bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_val0 = ot_bias_scan(golden_standart=golden_standart0, classifier=classifier0, data=X0, favorable_value=0, overpredicted=True, num_iters=1000000)\n",
    "ot_val1 = ot_bias_scan(golden_standart=golden_standart1, classifier=classifier1, data=X1, favorable_value=0, overpredicted=True, num_iters=1000000)\n",
    "bias1 = abs(ot_val0 - ot_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>ot_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex    ot_val\n",
       "0    0  0.000208\n",
       "1    1  0.001651"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bias = 0.0014426199440358929\n",
      "If the bias had been 0.3, there could be a mismatch of 30%.\n"
     ]
    }
   ],
   "source": [
    "bs1 = pd.DataFrame({\"sex\": [0, 1], \"ot_val\": [ot_val0, ot_val1]})\n",
    "display(bs1)\n",
    "print(\"Total bias = \", bias1, sep=\"\")\n",
    "print(\"If the bias had been 0.3, there could be a mismatch of 30%.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (\"race\" parameter)\n",
    "We'll train a simple classifier to predict the probability of the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "race\n",
      "age_cat\n",
      "priors_count\n",
      "c_charge_degree\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df3 = df[df['race'] == 0]\n",
    "df4 = df[df['race'] == 1]\n",
    "\n",
    "\n",
    "X3 = df3.drop('two_year_recid', axis = 1)\n",
    "golden_standart3 = df3['two_year_recid']\n",
    "clf3 = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2')\n",
    "clf3.fit(X3, golden_standart3)\n",
    "\n",
    "X4 = df4.drop('two_year_recid', axis = 1)\n",
    "golden_standart4 = df4['two_year_recid']\n",
    "clf4 = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2')\n",
    "clf4.fit(X4, golden_standart4)\n",
    "\n",
    "for col in X3.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = pd.Series(clf3.predict_proba(X3)[:,0])\n",
    "classifier4 = pd.Series(clf4.predict_proba(X4)[:,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring bias with respect to race\n",
    "Using ot_bias scan we obtained results evaluated for optimal transport between golden_standarts and classifiers. Their difference shows the measured bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_val3 = ot_bias_scan(golden_standart=golden_standart3, classifier=classifier3, data=X3, favorable_value=0, overpredicted=True, num_iters=1000000)\n",
    "ot_val4 = ot_bias_scan(golden_standart=golden_standart4, classifier=classifier4, data=X4, favorable_value=0, overpredicted=True, num_iters=1000000)\n",
    "bias2 = abs(ot_val3 - ot_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>ot_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex    ot_val\n",
       "0    0  0.000242\n",
       "1    1  0.000708"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bias = 0.000465750528936992\n",
      "If the bias had been 0.3, there could be a mismatch of 30%.\n"
     ]
    }
   ],
   "source": [
    "bs2 = pd.DataFrame({\"sex\": [0, 1], \"ot_val\": [ot_val3, ot_val4]})\n",
    "display(bs2)\n",
    "print(\"Total bias = \", bias2, sep=\"\")\n",
    "print(\"If the bias had been 0.3, there could be a mismatch of 30%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3a8dcdfdfc9ccb9b75f5bdad7d0512468824af451b906a037b07d69b0e56c16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
