{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Wasserstein distance using Optimal Transport (OT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Identifying Significant Predictive Bias in Classifiers\" \n",
    "https://arxiv.org/abs/1611.08292\n",
    "\n",
    "\"POT: Python Optimal Transport\" https://jmlr.org/papers/v22/20-451.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Transport (OT) is a field of mathematics which studies the geometry of probability spaces. Among its many contributions, OT provides a principled way to compare and align probability distributions by taking into account the underlying geometry of the\n",
    "considered metric space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Transport (OT) is a mathematical problem introduced by Gaspard Monge in 1781 that aim at finding the most efficient way to move mass between distributions. The cost of moving a unit of mass between two positions is called the ground cost and the objective is to minimize the overall cost of moving one mass distribution onto another one. The optimization problem can be expressed for two distributions $\\mu_s$ and $\\mu_t$ as\n",
    "$$\n",
    "\\min _{m, m \\# \\mu_s=\\mu_t} \\int c(x, m(x)) d \\mu_s(x),\n",
    "$$\n",
    "where $c(\\cdot, \\cdot)$ is the ground cost and the constraint $m \\# \\mu_s=\\mu_t$ ensures that $\\mu_s$ is completely transported to $\\mu_t$. This problem is particularly difficult to solve because of this constraint and has been replaced in practice (on discrete distributions) by a linear program easier to solve. It corresponds to the Kantorovitch formulation where the Monge mapping $m$ is replaced by a joint distribution (OT matrix expressed in the next section) (see Solving optimal transport).\n",
    "From the optimization problem above we can see that there are two main aspects to the OT solution that can be used in practical applications:\n",
    "- The optimal value (Wasserstein distance): Measures similarity between distributions.\n",
    "- The optimal mapping (Monge mapping, OT matrix): Finds correspondences between distributions.\n",
    "\n",
    "In the first case, OT can be used to measure similarity between distributions (or datasets), in this case the Wasserstein distance (the optimal value of the problem) is used. In the second case one can be interested in the way the mass is moved between the distributions (the mapping). This mapping can then be used to transfer knowledge between distributions.\n",
    "In the first case, OT can be used to measure similarity between distributions (or datasets), in this case the Wasserstein distance (the optimal value of the problem) is used. In the second case one can be interested in the way the mass is moved between the distributions (the mapping). This mapping can then be used to transfer knowledge between distributions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein distance between distributions\n",
    "\n",
    "OT is often used to measure similarity between distributions, especially when they do not share the same support. When the support between the distributions is disjoint OT-based Wasserstein distances compare favorably to popular f-divergences including the popular Kullback-Leibler, Jensen-Shannon divergences, and the Total Variation distance. What is particularly interesting for data science applications is that one can compute meaningful sub-gradients of the Wasserstein distance. For these reasons it became a very efficient tool for machine learning applications that need to measure and optimize similarity between empirical distributions.\n",
    "\n",
    "Numerous contributions make use of this an approach is the machine learning (ML) literature. For example OT was used for training Generative Adversarial Networks (GANs) in order to overcome the vanishing gradient problem. It has also been used to find discriminant or robust subspaces for a dataset. The Wasserstein distance has also been used to measure similarity between word embeddings of documents or between signals or spectra."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OT for mapping estimation\n",
    "\n",
    "A very interesting aspect of OT problem is the OT mapping in itself. When computing optimal transport between discrete distributions one output is the OT matrix that will provide you with correspondences between the samples in each distributions.\n",
    "\n",
    "This correspondence is estimated with respect to the OT criterion and is found in a non-supervised way, which makes it very interesting on problems of transfer between datasets. It has been used to perform color transfer between images or in the context of domain adaptation. More recent applications include the use of extension of OT (Gromov-Wasserstein) to find correspondences between languages in word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kantorovich optimal transport problem\n",
    "\n",
    "This is the most typical OT problem. It seeks an optimal coupling $\\boldsymbol{T}$ which minimizes the displacement cost of a discrete measure $\\boldsymbol{a}$ to a discrete measure $\\boldsymbol{b}$ with respect to a ground cost $\\boldsymbol{M} \\in \\mathbb{R}^{n_{1} \\times n_{2}}$. In order to be a transport plan, $\\boldsymbol{T}$ must be part of the set $\\Pi(\\mathbf{a}, \\mathbf{b})=\\left\\{\\boldsymbol{T} \\geq \\mathbf{0}, \\boldsymbol{T} \\mathbf{1}_{n_{2}}=\\boldsymbol{a}, \\boldsymbol{T}^{\\top} \\mathbf{1}_{n_{1}}=\\boldsymbol{b}\\right\\}$. When the ground cost is a metric, the optimal value of the OT problem is also a metric (Rubner et al., 2000; Cuturi and Avis, 2014) and is called the Wasserstein distance. In this discrete case, the OT problem is defined as\n",
    "\n",
    "$$\n",
    "W_{M}(\\boldsymbol{a}, \\boldsymbol{b})=\\min _{\\boldsymbol{T} \\in \\Pi(\\mathbf{a}, \\mathbf{b})}\\langle\\boldsymbol{T}, \\boldsymbol{M}\\rangle\n",
    "$$\n",
    "\n",
    "which is a linear program. The optimization problem above is often adapted to include a regularization term for the transport plan $\\boldsymbol{T}$, such as entropic regularization (Cuturi, 2013) or squared L2. For the entropic regularized OT problem, one may use the Sinkhorn Knopp algorithm (or variants), or stochastic optimization algorithms. POT has a simple syntax to solve these problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving optimal transport\n",
    "\n",
    "The optimal transport problem between discrete distributions is often expressed as\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\gamma^*=\\arg \\min _{\\gamma \\in \\mathbb{R}_{+}^{m \\times n}} \\sum_{i, j} \\gamma_{i, j} M_{i, j} \\\\\n",
    "\\text { s.t. } \\gamma 1=a ; \\gamma^T 1=b ; \\gamma \\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "where:\n",
    "- $M \\in \\mathbb{R}_{+}^{m \\times n}$ is the metric cost matrix defining the cost to move mass from bin $a_i$ to bin $b_j$.\n",
    "- $a$ and $b$ are histograms on the simplex (positive, sum to 1) that represent the weights of each samples in the source an target distributions.\n",
    "Solving the linear program above can be done using the function ot.emd that will return the optimal transport matrix $\\gamma^*$ :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The necessity of usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between the MDSS and OT detectors is, the range of applicability. They can tell real quick (linear time) what the most anomalous subset is, while in the case of OT detector we are not that ambitious yet. The method used (ot.emd) can only account for 1 dimensional histograms as can be seen in \"POT: Python Optimal Transport\" https://jmlr.org/papers/v22/20-451.html. Which means we cannot yet handle datasets of more than 1 feature, therefore it is a sort of toy example to build something bigger in the next steps. For more dimensions we may use in the future the Sinkhorn algorithm.\n",
    "\n",
    "The matrix we get from the emd method is called a transport plan in the OT framework (the gamma matrix in https://pythonot.github.io/all.html?highlight=emd#ot.emd). It is the matrix which minimizes the transportation cost, from it we can have the actual Wasserstein distance by taking the Frobenius product (see here for instance: https://en.wikipedia.org/wiki/Frobenius_inner_product) with the metric cost matrix, M. We could test in our case (1 dimension) if we get the same by computing ot.wasserstein_1d, although that we may change the way the matrix M is currently defined (as it is now taking into account only the difference between the indices of the matrix)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Optimal Transport supports only scoring function *Optimal Transport*.\n",
    "Note, non-parametric scoring functions can only be used for datasets where the expectations are constant or none.\n",
    "\n",
    "The type of outcomes must be provided using the mode keyword argument. The definition for the four types of outcomes supported are provided below:\n",
    "- Ordinal: Multiclass outcomes that are ranked in a specific order. Outcomes must be positive integers.\n",
    "- Binary: Yes/no outcomes. Outcomes must 0 or 1.\n",
    "- Continuous: Continuous outcomes. Outcomes could be any real number.\n",
    "- Nominal: Multiclass outcomes with no rank or order between them. Outcomes must be a finite set of integers with dimensionality <= 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.detectors.ot_detector import ot_bias_scan\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll demonstrate finding the optimal transport matrix with ot_bias_scan using the compas dataset. We can specify subgroups to be scored or scan for the most anomalous subgroup. Bias scan allows us to decide if we aim to identify bias as `higher` than expected probabilities or `lower` than expected probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compas Dataset\n",
    "This is a binary classification use case where the favorable label is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "dataset_orig = load_preproc_data_compas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the categorical features one-hot encoded so we'll modify the dataset to convert them back \n",
    "to the categorical featues because scanning one-hot encoded features may find subgroups that are not meaningful eg. a subgroup with 2 race values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_df = pd.DataFrame(dataset_orig.features, columns=dataset_orig.feature_names)\n",
    "\n",
    "age_cat = np.argmax(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45', \n",
    "                                     'age_cat=Greater than 45']].values, axis=1).reshape(-1, 1)\n",
    "priors_count = np.argmax(dataset_orig_df[['priors_count=0', 'priors_count=1 to 3', \n",
    "                                          'priors_count=More than 3']].values, axis=1).reshape(-1, 1)\n",
    "c_charge_degree = np.argmax(dataset_orig_df[['c_charge_degree=F', 'c_charge_degree=M']].values, axis=1).reshape(-1, 1)\n",
    "\n",
    "features = np.concatenate((dataset_orig_df[['sex', 'race']].values, age_cat, priors_count, \\\n",
    "                           c_charge_degree, dataset_orig.labels), axis=1)\n",
    "feature_names = ['sex', 'race', 'age_cat', 'priors_count', 'c_charge_degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features, columns=feature_names + ['two_year_recid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  race  age_cat  priors_count  c_charge_degree  two_year_recid\n",
       "0  0.0   0.0      1.0           0.0              0.0             1.0\n",
       "1  0.0   0.0      0.0           2.0              0.0             1.0\n",
       "2  0.0   1.0      1.0           2.0              0.0             1.0\n",
       "3  1.0   1.0      1.0           0.0              1.0             0.0\n",
       "4  0.0   1.0      1.0           0.0              0.0             0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We'll train a simple classifier to predict the probability of the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X = df.drop('two_year_recid', axis = 1)\n",
    "y = df['two_year_recid']\n",
    "clf = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the probability scores we use are the probabilities of the favorable label, which is 0 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sex  race  age_cat  priors_count  c_charge_degree\n",
      "0     0.0   0.0      1.0           0.0              0.0\n",
      "1     0.0   0.0      0.0           2.0              0.0\n",
      "2     0.0   1.0      1.0           2.0              0.0\n",
      "3     1.0   1.0      1.0           0.0              1.0\n",
      "4     0.0   1.0      1.0           0.0              0.0\n",
      "...   ...   ...      ...           ...              ...\n",
      "5273  0.0   0.0      1.0           0.0              1.0\n",
      "5274  0.0   0.0      0.0           0.0              0.0\n",
      "5275  0.0   0.0      0.0           0.0              0.0\n",
      "5276  0.0   0.0      0.0           0.0              0.0\n",
      "5277  1.0   0.0      1.0           1.0              1.0\n",
      "\n",
      "[5278 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "probs = pd.Series(clf.predict_proba(X)[:,0])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Earth Mover's Distance value\n",
    "We can scan for an optimal transport using ot_bias_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_val = ot_bias_scan(observations=y, ideal_distribution=probs, data=X, favorable_value=0, overpredicted=True, num_iters=1000000, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66223.6459584531\n"
     ]
    }
   ],
   "source": [
    "print(ot_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3a8dcdfdfc9ccb9b75f5bdad7d0512468824af451b906a037b07d69b0e56c16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
